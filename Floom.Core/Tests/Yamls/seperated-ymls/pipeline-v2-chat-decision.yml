name: Floom Pipeline

jobs:
  execute:
    steps:
      - name: Execute LLM Inference
        uses: floom/functions/chat-decision-tree
        with:
          user: ${{ this.prompt }}
          system: "History: ${{ this.chat_history }} >> 
                  Given a chat history and the latest user question \
                  which might reference context in the chat history, formulate a standalone question \
                  which can be understood without the chat history. Do NOT answer the question, \
                  just reformulate it if needed and otherwise return it as is"
        output:
          model_output: ${{ result }}

      - name: Response Formatter
        uses: floom/functions/response-formatter
        with:
          type: object
          structure: {
            "decision_output": "standalone or history",
            "prompt": "prompt returned from inference"
          }